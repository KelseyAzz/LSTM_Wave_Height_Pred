import os
import pandas as pd
import numpy as np
from math import sqrt
from matplotlib import pyplot as plt
from pandas import read_csv, DataFrame, concat
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from sklearn.feature_selection import mutual_info_regression, SelectKBest
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, InputLayer
from keras.callbacks import ModelCheckpoint
from keras.losses import MeanSquaredError
from keras.metrics import RootMeanSquaredError
from keras.optimizers import Adam
from datetime import datetime
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_absolute_percentage_error

def time_series_to_supervised(data, n_lag=5, n_fut=20, selLag=None, selFut=None, dropnan=True):
    """
    Converts a time series to a supervised learning data set by adding time-shifted prior and future period
    data as input or output (i.e., target result) columns for each period
    :param data:  a series of periodic attributes as a list or NumPy array
    :param n_lag: number of PRIOR periods to lag as input (X); generates: X(t-1), X(t-2); min= 0 --> nothing lagged
    :param n_fut: number of FUTURE periods to add as target output (y); generates Yout(t+1); min= 0 --> no future periods
    :param selLag:  only copy these specific PRIOR period attributes; default= None; EX: ['Xa', 'Xb' ]
    :param selFut:  only copy these specific FUTURE period attributes; default= None; EX: ['rslt', 'xx']
    :param dropnan: True= drop rows with NaN values; default= True
    :return: a Pandas DataFrame of time series data organized for supervised learning
    NOTES:
    (1) The current period's data is always included in the output.
    (2) A suffix is added to the original column names to indicate a relative time reference: e.g., (t) is the current
        period; (t-2) is from two periods in the past; (t+1) is from the next period
    """
    n_vars = 1 if type(data) is list else data.shape[1]
    df = DataFrame(data)
    origNames = df.columns
    cols, names = list(), list()
    # include all current period attributes
    cols.append(df.shift(0))
    names += [('%s' % origNames[j]) for j in range(n_vars)]
    # lag any past period attributes (t-n_lag,...,t-1)
    n_lag = max(0, n_lag)  # force valid number of lag periods
    for i in range(n_lag, 0, -1):
        suffix= '(t-%d)' % i
        if (None == selLag):   
            cols.append(df.shift(i))
            names += [('%s%s' % (origNames[j], suffix)) for j in range(n_vars)]
        else:
            for var in (selLag):
                cols.append(df[var].shift(i))
                names+= [('%s%s' % (var, suffix))]
 
    # include future period attributes (t+1,...,t+n_fut)
    n_fut = max(n_fut, 0)  # force valid number of future periods to shift back
    for i in range(1, n_fut + 1):
        suffix= '(t+%d)' % i
        if (None == selFut):  
            cols.append(df.shift(-i))
            names += [('%s%s' % (origNames[j], suffix)) for j in range(n_vars)]
        else:  # copy only selected future attributes
            for var in (selFut):
                cols.append(df[var].shift(-i))
                names += [('%s%s' % (var, suffix))]
    # combine everything
    agg = concat(cols, axis=1)
    agg.columns = names
    # drop rows with NaN values introduced by lagging
    if dropnan:
        agg.dropna(inplace=True)
    return agg

# Ensure TensorFlow uses CPU if desired
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

def parser(x):
    return datetime.strptime(x, '%d/%m/%Y %H:%M')

# Load the dataset with the custom date parser
df = pd.read_csv('~/Downloads/Data_Final.csv', index_col='Date_time', parse_dates=['Date_time'], date_parser=parser)
print(df.columns)
df.drop(df.columns[[0]], axis=1, inplace=True)
values = df.values
# ensure all data is float
values = values.astype('float32')
# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
# specify the number of lag minutes
n_hours = 5  #number of prior minutes which are going to be taken into consideration
n_features = 5   #number of variables which are present in the dataset 
# frame as supervised learning
reframed = time_series_to_supervised(scaled,n_hours,1)   #vary the last value to 1,5,10,20 to obtain the one, five, ten and twenty step ahead forecasts 


# split into train and test sets
values = reframed.values
n_train_hours = 8000 
train = values[:n_train_hours, :]
test = values[n_train_hours:, :] 
# split into input and outputs
n_obs = n_hours * n_features   
train_X, train_y = train[:, :n_obs], train[:, -n_features] 
test_X, test_y = test[:, :n_obs], test[:, -n_features]
print(train_X.shape, len(train_X), train_y.shape)
# reshape input to be 3D [samples, timesteps, features]
train_X = train_X.reshape((train_X.shape[0], n_hours, n_features))
test_X = test_X.reshape((test_X.shape[0], n_hours, n_features))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)


# reshape input to be 2D [samples, timesteps*features] for feature selection
train_X_flat = train_X.reshape((train_X.shape[0], n_hours * n_features))
test_X_flat = test_X.reshape((test_X.shape[0], n_hours * n_features))


from sklearn.feature_selection import mutual_info_regression 
# feature selection using mutual information
f_selector = SelectKBest(score_func=mutual_info_regression, k='all')
# learn relationship from training data
f_selector.fit(train_X_flat, train_y)
# transform train input data
X_train_fs = f_selector.transform(train_X_flat)
# Obtaining the scores for the features
f_selector.scores_


# design network
from keras.models import load_model
from keras.callbacks import ModelCheckpoint
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
model = Sequential()
model.add(LSTM(10, input_shape=(train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss='mse', optimizer='adam')
es = EarlyStopping(monitor='val_loss', min_delta=4, verbose=1, patience=10)
# fit network
history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y),verbose=2,callbacks=[es], shuffle=False)
model.summary()

# plot history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.xlim([0, 10])
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend()
plt.show()

# make a prediction
yhat = model.predict(test_X)
test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))
# invert scaling for forecast
inv_yhat = np.concatenate((yhat, test_X[:, -4:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]
# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = np.concatenate((test_y, test_X[:, -4:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]

# calculate RMSE
rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)
from sklearn.metrics import mean_absolute_error
MAE = mean_absolute_error(inv_y, inv_yhat)
MAE
print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')
mape = mean_absolute_percentage_error(inv_y, inv_yhat)


# Plot actual vs predicted
plt.figure(figsize=(15, 6))
plt.plot(inv_y, label='Actual')
plt.plot(inv_yhat, label='Predicted')
plt.ylabel('Sea Level')
plt.xlabel('Time Interval')
plt.legend()
plt.show()

#Additional plots
#plots of the differences between the acutal and predicted values
difference=((inv_y))-inv_yhat
plt.plot(difference, label='difference between actual and predicted')  #seiche event november 2021
plt.ylabel('sea level')
plt.xlabel('time interval')
plt.legend()
plt.show()


#rolling variance of the predicted values
window = 20  # size of the window
A =inv_yhat
Aw = np.lib.stride_tricks.sliding_window_view(A, window)
Avar = np.var(Aw, axis=-1)
Avar
plt.plot(Avar, label='20 min rolling variance for the predicted values') 
plt.ylabel('rolling variance for sea level')
plt.xlabel('time interval')
plt.legend()


# Plot the actual vs. predicted values
plt.figure(figsize=(14, 7))

# Plot actual values
plt.plot(inv_y, label='Actual Values')

# Plot predicted values
plt.plot(A, label='Predicted Values', alpha=0.7)

# Plot the rolling variance on the same plot but with a secondary y-axis
ax1 = plt.gca()  # get the current axis
#ax2 = ax1.twinx()  # create a secondary y-axis

# Plot rolling variance
#ax2.plot(range(window - 1, len(A)), Avar, label='20 min rolling variance', color='green', linestyle='--')

# Labels and legends
ax1.set_xlabel('Time Interval')
ax1.set_ylabel('Sea Level')
#ax2.set_ylabel('Rolling Variance')

ax1.legend(loc='upper left')
#ax2.legend(loc='upper right')

plt.title('Actual vs Predicted Values with Rolling Variance')
plt.show()

